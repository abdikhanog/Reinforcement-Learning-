{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download imprtant packages\n",
    "import numpy as np\n",
    "import pandas as pds\n",
    "import random\n",
    "import copy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from collections import deque\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(30) #############################generate a random maze, with size 10 by 10.\n",
    "class Maze(object):# ########################this is the class maze object, i.e. we create a random maze\n",
    "    def __init__(self, size=10, blocks_rate=0.2):\n",
    "        self.size = size if size > 3 else 10\n",
    "        self.blocks = int((size ** 2) * blocks_rate) \n",
    "        self.s_list = []\n",
    "        self.maze_list = []\n",
    "        self.e_list = []\n",
    "\n",
    "    def create_mid_lines(self, k):#########This is where we create hedges in the maze, i.e. 'X'\n",
    "        if k == 0: self.maze_list.append(self.s_list)\n",
    "        elif k == self.size - 1: self.maze_list.append(self.e_list)\n",
    "        else:\n",
    "            tmp_list = []\n",
    "            for l in range(0,self.size):\n",
    "                if l == 0: tmp_list.extend(\"X\")\n",
    "                elif l == self.size-1: tmp_list.extend(\"X\")\n",
    "                else:\n",
    "                    a = random.randint(-1, 0)####### generate random -1 and 0 rewards\n",
    "                    tmp_list.extend([a])\n",
    "            self.maze_list.append(tmp_list)\n",
    "\n",
    "    def insert_blocks(self, k, s_r, e_r):######inserting 'x' into the maze.\n",
    "        b_y = random.randint(1, self.size-2)\n",
    "        b_x = random.randint(1, self.size-2)\n",
    "        if [b_y, b_x] == [1, s_r] or [b_y, b_x] == [self.size - 2, e_r]: k = k-1\n",
    "        else: self.maze_list[b_y][b_x] = \"X\"\n",
    "            \n",
    "    def generate_maze(self): ######## this sets up entry cell to maze and assigns '#'\n",
    "        s_r = random.randint(1, (self.size / 2) - 1)\n",
    "        for i in range(0, self.size):\n",
    "            if i == s_r: self.s_list.extend(\"#\")\n",
    "            else: self.s_list.extend(\"X\")\n",
    "        start_point = [0, s_r]\n",
    "\n",
    "        e_r = random.randint((self.size / 2) + 1, self.size - 2)\n",
    "        for j in range(0, self.size):\n",
    "            if j == e_r: self.e_list.extend([100])\n",
    "            else: self.e_list.extend(\"X\")\n",
    "        goal_point = [self.size - 1, e_r]\n",
    "\n",
    "        for k in range(0, self.size):\n",
    "            self.create_mid_lines(k)\n",
    "        \n",
    "        for k in range(self.blocks):\n",
    "            self.insert_blocks(k, s_r, e_r)\n",
    "\n",
    "        return self.maze_list, start_point, goal_point\n",
    "random.seed(30)#############################################this is done to reproduce maze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(30)############this is where we introduce agent and reward system \n",
    "class Field(object):\n",
    "    def __init__(self, maze, start_point, goal_point):\n",
    "        self.maze = maze\n",
    "        self.start_point = start_point\n",
    "        self.goal_point = goal_point\n",
    "        self.movable_vec = [[1,0],[-1,0],[0,1],[0,-1]]\n",
    "\n",
    "    def display(self, point=None):\n",
    "        field_data = copy.deepcopy(self.maze)\n",
    "        if not point is None:\n",
    "                y, x = point\n",
    "                field_data[y][x] = \"@\"\n",
    "        else:\n",
    "                point = \"\"\n",
    "        for line in field_data:\n",
    "                print (\"\\t\" + \"%3s \" * len(line) % tuple(line))\n",
    "\n",
    "    def get_actions(self, state):####### this defines actions agent can make, it should avoid 'x' \n",
    "        movables = []\n",
    "        if state == self.start_point:\n",
    "            y = state[0] + 1\n",
    "            x = state[1]\n",
    "            a = [[y, x]]\n",
    "            return a\n",
    "        else:\n",
    "            for v in self.movable_vec:\n",
    "                y = state[0] + v[0]\n",
    "                x = state[1] + v[1]\n",
    "                if not(0 < x < len(self.maze) and\n",
    "                       0 <= y <= len(self.maze) - 1 and\n",
    "                       maze[y][x] != \"X\" and\n",
    "                       maze[y][x] != \"#\"):\n",
    "                    continue\n",
    "                movables.append([y,x])\n",
    "            if len(movables) != 0:\n",
    "                return movables\n",
    "            else:\n",
    "                return None\n",
    "\n",
    "    def get_val(self, state):\n",
    "        y, x = state\n",
    "        if state == self.start_point: return 0, False\n",
    "        else:\n",
    "            v = float(self.maze[y][x])\n",
    "            if state == self.goal_point: \n",
    "                return v, True\n",
    "            else: \n",
    "                return v, False\n",
    "\n",
    "random.seed(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t  X   X   X   #   X   X   X   X   X   X \n",
      "\t  X  -1  -1   0   X   X   0  -1  -1   X \n",
      "\t  X   0  -1  -1  -1  -1   X   0   0   X \n",
      "\t  X  -1   0   X  -1   0   0   X   X   X \n",
      "\t  X  -1   0   0  -1   0   0   0  -1   X \n",
      "\t  X  -1  -1   0  -1  -1   X   0   X   X \n",
      "\t  X  -1   0   0   0   X   0  -1  -1   X \n",
      "\t  X   X   X   0   0   X   0   X   X   X \n",
      "\t  X   X  -1   X   0  -1  -1   0   0   X \n",
      "\t  X   X   X   X   X   X   X   X 100   X \n"
     ]
    }
   ],
   "source": [
    "random.seed(30)# this builds the maze and displays it\n",
    "size = 10\n",
    "barriar_rate = 0.2\n",
    "\n",
    "maze_1 = Maze(size, barriar_rate)\n",
    "maze, start_point, goal_point = maze_1.generate_maze()\n",
    "maze_field = Field(maze, start_point, goal_point)\n",
    "\n",
    "maze_field.display()\n",
    "random.seed(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(30)\n",
    "class QLearning_Solver(object):########## this builds Q-learning algorithm and defines all the parameters, such alpha and so on...\n",
    "    def __init__(self, maze, display=False):\n",
    "        self.Qvalue = {}\n",
    "        self.Field = maze\n",
    "        self.alpha = 0.6\n",
    "        self.gamma  = 0.7\n",
    "        self.epsilon = 0.8\n",
    "        self.steps = 0\n",
    "        self.score = 0\n",
    "        self.display = display\n",
    "\n",
    "    def qlearn(self, greedy_flg=False):####### this defines the reward system and how it is accumulated and also defines the step sizes \n",
    "        state = self.Field.start_point \n",
    "        while True:\n",
    "            if greedy_flg:\n",
    "                self.steps += 1\n",
    "                action = self.choose_action_greedy(state)\n",
    "                print(\"current state: {0} -> action: {1} \".format(state, action))# this sets the states and action and displays it\n",
    "                if self.display:\n",
    "                    self.Field.display(action)\n",
    "                reward, tf = self.Field.get_val(action)\n",
    "                self.score =  self.score + reward\n",
    "                print(\"current step: {0} \\t score: {1}\\n\".format(self.steps, self.score))\n",
    "                if tf == True:\n",
    "                    print(\"Goal!\")############# print goal once algorithm converges for greedy policy\n",
    "                    break\n",
    "            else:\n",
    "                action = self.choose_action(state)    \n",
    "            if self.update_Qvalue(state, action):\n",
    "                break\n",
    "            else:\n",
    "                state = action\n",
    "\n",
    "    def update_Qvalue(self, state, action):#### defines and calculates the Q-learning algorithm\n",
    "        Q_s_a = self.get_Qvalue(state, action)\n",
    "        mQ_s_a = max([self.get_Qvalue(action, n_action) for n_action in self.Field.get_actions(action)])\n",
    "        r_s_a, finish_flg = self.Field.get_val(action)\n",
    "        q_value = Q_s_a + self.alpha * ( r_s_a +  self.gamma * mQ_s_a - Q_s_a)\n",
    "        self.set_Qvalue(state, action, q_value)\n",
    "        return finish_flg\n",
    "\n",
    "\n",
    "    def get_Qvalue(self, state, action):### get q-value for each state and action pair\n",
    "        state = (state[0],state[1])\n",
    "        action = (action[0],action[1])\n",
    "        try:\n",
    "            return self.Qvalue[state][action]\n",
    "        except KeyError:\n",
    "            return 0.0\n",
    "\n",
    "    def set_Qvalue(self, state, action, q_value):\n",
    "        state = (state[0],state[1])\n",
    "        action = (action[0],action[1])\n",
    "        self.Qvalue.setdefault(state,{})\n",
    "        self.Qvalue[state][action] = q_value\n",
    "\n",
    "    def choose_action(self, state):### define and implement epsilon/greedy policy\n",
    "        if self.epsilon < random.random():\n",
    "            return random.choice(self.Field.get_actions(state))\n",
    "        else:\n",
    "            return self.choose_action_greedy(state)\n",
    "\n",
    "    def choose_action_greedy(self, state):\n",
    "        best_actions = []\n",
    "        max_q_value = -50\n",
    "        for a in self.Field.get_actions(state):\n",
    "            q_value = self.get_Qvalue(state, a)\n",
    "            if q_value > max_q_value:\n",
    "                best_actions = [a,]\n",
    "                max_q_value = q_value\n",
    "            elif q_value == max_q_value:\n",
    "                best_actions.append(a)\n",
    "        return random.choice(best_actions)\n",
    "\n",
    "    def dump_Qvalue(self):########print each Q-value for each state and action\n",
    "        print(\"##### Dump Qvalue #####\")\n",
    "        for i, s in enumerate(self.Qvalue.keys()):\n",
    "            for a in self.Qvalue[s].keys():\n",
    "                print(\"\\t\\tQ(s, a): Q(%s, %s): %s\" % (str(s), str(a), str(self.Qvalue[s][a])))\n",
    "            if i != len(self.Qvalue.keys())-1: \n",
    "                print('\\t------------------state   action   reward')\n",
    "        \n",
    "random.seed(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Dump Qvalue #####\n",
      "\t\tQ(s, a): Q((0, 3), (1, 3)): -1.059101797404672\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((1, 3), (2, 3)): -1.6291077605375999\n",
      "\t\tQ(s, a): Q((1, 3), (1, 2)): -1.6314128161075199\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((2, 3), (1, 3)): -0.997482544128\n",
      "\t\tQ(s, a): Q((2, 3), (2, 4)): -1.2888\n",
      "\t\tQ(s, a): Q((2, 3), (2, 2)): -0.98976\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((1, 2), (1, 3)): -1.025075743888896\n",
      "\t\tQ(s, a): Q((1, 2), (1, 1)): -1.04184\n",
      "\t\tQ(s, a): Q((1, 2), (2, 2)): -0.9743999999999999\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((1, 1), (2, 1)): -0.4536\n",
      "\t\tQ(s, a): Q((1, 1), (1, 2)): -0.9144576677759999\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((2, 1), (3, 1)): -0.84\n",
      "\t\tQ(s, a): Q((2, 1), (1, 1)): -0.84\n",
      "\t\tQ(s, a): Q((2, 1), (2, 2)): -0.9359999999999999\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((3, 1), (3, 2)): 0.0\n",
      "\t\tQ(s, a): Q((3, 1), (2, 1)): -0.252\n",
      "\t\tQ(s, a): Q((3, 1), (4, 1)): -0.6\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((3, 2), (2, 2)): -0.9983616\n",
      "\t\tQ(s, a): Q((3, 2), (4, 2)): 0.0\n",
      "\t\tQ(s, a): Q((3, 2), (3, 1)): -0.98976\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((2, 2), (2, 3)): -1.4299199999999999\n",
      "\t\tQ(s, a): Q((2, 2), (3, 2)): 0.0\n",
      "\t\tQ(s, a): Q((2, 2), (1, 2)): -0.70584\n",
      "\t\tQ(s, a): Q((2, 2), (2, 1)): -0.252\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((2, 4), (2, 5)): -0.9359999999999999\n",
      "\t\tQ(s, a): Q((2, 4), (3, 4)): -0.84\n",
      "\t\tQ(s, a): Q((2, 4), (2, 3)): -1.3614632371199997\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((2, 5), (2, 4)): -1.2888\n",
      "\t\tQ(s, a): Q((2, 5), (3, 5)): 0.0\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((3, 5), (3, 6)): 0.0\n",
      "\t\tQ(s, a): Q((3, 5), (2, 5)): -0.99934464\n",
      "\t\tQ(s, a): Q((3, 5), (3, 4)): -0.9999932891136\n",
      "\t\tQ(s, a): Q((3, 5), (4, 5)): 0.0\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((3, 6), (4, 6)): 0.0\n",
      "\t\tQ(s, a): Q((3, 6), (3, 5)): 0.0\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((4, 6), (4, 5)): 0.0\n",
      "\t\tQ(s, a): Q((4, 6), (4, 7)): 0.0\n",
      "\t\tQ(s, a): Q((4, 6), (3, 6)): 0.0\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((4, 5), (3, 5)): 0.0\n",
      "\t\tQ(s, a): Q((4, 5), (5, 5)): -0.999983222784\n",
      "\t\tQ(s, a): Q((4, 5), (4, 6)): 0.0\n",
      "\t\tQ(s, a): Q((4, 5), (4, 4)): -0.9983616\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((3, 4), (3, 5)): 0.0\n",
      "\t\tQ(s, a): Q((3, 4), (2, 4)): -0.6\n",
      "\t\tQ(s, a): Q((3, 4), (4, 4)): -0.6\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((5, 5), (5, 4)): -0.84\n",
      "\t\tQ(s, a): Q((5, 5), (4, 5)): 0.0\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((5, 4), (5, 3)): 0.0\n",
      "\t\tQ(s, a): Q((5, 4), (4, 4)): -0.9743999999999999\n",
      "\t\tQ(s, a): Q((5, 4), (5, 5)): -0.84\n",
      "\t\tQ(s, a): Q((5, 4), (6, 4)): 0.0\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((5, 3), (5, 4)): -0.999998926258176\n",
      "\t\tQ(s, a): Q((5, 3), (4, 3)): 0.0\n",
      "\t\tQ(s, a): Q((5, 3), (6, 3)): 0.0\n",
      "\t\tQ(s, a): Q((5, 3), (5, 2)): -0.9999932891136\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((4, 4), (3, 4)): -0.9359999999999999\n",
      "\t\tQ(s, a): Q((4, 4), (5, 4)): -0.98976\n",
      "\t\tQ(s, a): Q((4, 4), (4, 5)): 0.0\n",
      "\t\tQ(s, a): Q((4, 4), (4, 3)): 0.0\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((4, 7), (5, 7)): 0.0\n",
      "\t\tQ(s, a): Q((4, 7), (4, 6)): 0.0\n",
      "\t\tQ(s, a): Q((4, 7), (4, 8)): -0.9999999956019535\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((5, 7), (4, 7)): 0.0\n",
      "\t\tQ(s, a): Q((5, 7), (6, 7)): -0.999998926258176\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((4, 3), (4, 2)): 0.0\n",
      "\t\tQ(s, a): Q((4, 3), (4, 4)): -0.999737856\n",
      "\t\tQ(s, a): Q((4, 3), (5, 3)): 0.0\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((4, 2), (4, 1)): -0.99995805696\n",
      "\t\tQ(s, a): Q((4, 2), (5, 2)): -0.98976\n",
      "\t\tQ(s, a): Q((4, 2), (3, 2)): 0.0\n",
      "\t\tQ(s, a): Q((4, 2), (4, 3)): 0.0\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((4, 1), (4, 2)): 0.0\n",
      "\t\tQ(s, a): Q((4, 1), (5, 1)): -0.84\n",
      "\t\tQ(s, a): Q((4, 1), (3, 1)): -0.84\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((5, 2), (5, 1)): -1.2888\n",
      "\t\tQ(s, a): Q((5, 2), (5, 3)): 0.0\n",
      "\t\tQ(s, a): Q((5, 2), (6, 2)): 0.0\n",
      "\t\tQ(s, a): Q((5, 2), (4, 2)): 0.0\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((5, 1), (4, 1)): -0.84\n",
      "\t\tQ(s, a): Q((5, 1), (5, 2)): -0.84\n",
      "\t\tQ(s, a): Q((5, 1), (6, 1)): -0.84\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((4, 8), (4, 7)): 0.0\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((6, 7), (6, 8)): -0.6\n",
      "\t\tQ(s, a): Q((6, 7), (6, 6)): 0.0\n",
      "\t\tQ(s, a): Q((6, 7), (5, 7)): 0.0\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((6, 8), (6, 7)): -0.6\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((6, 6), (6, 7)): -0.99995805696\n",
      "\t\tQ(s, a): Q((6, 6), (7, 6)): 1.4477057279999994\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((7, 6), (8, 6)): 10.558383359999997\n",
      "\t\tQ(s, a): Q((7, 6), (6, 6)): 0.0\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((8, 6), (8, 7)): 47.25307238399999\n",
      "\t\tQ(s, a): Q((8, 6), (7, 6)): 0.0\n",
      "\t\tQ(s, a): Q((8, 6), (8, 5)): -0.6\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((8, 7), (8, 8)): 69.6100608\n",
      "\t\tQ(s, a): Q((8, 7), (8, 6)): 13.207084799999997\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((8, 8), (8, 7)): 28.19577599999999\n",
      "\t\tQ(s, a): Q((8, 8), (9, 8)): 99.934464\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((8, 5), (8, 4)): 4.680759334809598\n",
      "\t\tQ(s, a): Q((8, 5), (8, 6)): 27.675963033599995\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((8, 4), (8, 5)): 13.492870966271997\n",
      "\t\tQ(s, a): Q((8, 4), (7, 4)): 0.0\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((7, 4), (7, 3)): 0.0\n",
      "\t\tQ(s, a): Q((7, 4), (6, 4)): 0.0\n",
      "\t\tQ(s, a): Q((7, 4), (8, 4)): 2.2151431572479985\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((7, 3), (6, 3)): 0.0\n",
      "\t\tQ(s, a): Q((7, 3), (7, 4)): 0.0\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((6, 3), (5, 3)): 0.0\n",
      "\t\tQ(s, a): Q((6, 3), (6, 4)): 0.0\n",
      "\t\tQ(s, a): Q((6, 3), (7, 3)): 0.0\n",
      "\t\tQ(s, a): Q((6, 3), (6, 2)): 0.0\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((6, 4), (6, 3)): 0.0\n",
      "\t\tQ(s, a): Q((6, 4), (7, 4)): 0.0\n",
      "\t\tQ(s, a): Q((6, 4), (5, 4)): -0.99934464\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((6, 1), (5, 1)): -0.852\n",
      "\t\tQ(s, a): Q((6, 1), (6, 2)): 0.0\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((6, 2), (6, 1)): -0.9983616\n",
      "\t\tQ(s, a): Q((6, 2), (5, 2)): -0.9743999999999999\n",
      "\t\tQ(s, a): Q((6, 2), (6, 3)): 0.0\n"
     ]
    }
   ],
   "source": [
    "random.seed(30)\n",
    "learning_count = 8 ### epoch, how many run through agent makes in the maze\n",
    "QL_solver = QLearning_Solver(maze_field, display=True)\n",
    "for i in range(learning_count):\n",
    "    QL_solver.qlearn()\n",
    "\n",
    "QL_solver.dump_Qvalue()\n",
    "\n",
    "random.seed(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current state: [0, 3] -> action: [1, 3] \n",
      "\t  X   X   X   #   X   X   X   X   X   X \n",
      "\t  X  -1  -1   @   X   X   0  -1  -1   X \n",
      "\t  X   0  -1  -1  -1  -1   X   0   0   X \n",
      "\t  X  -1   0   X  -1   0   0   X   X   X \n",
      "\t  X  -1   0   0  -1   0   0   0  -1   X \n",
      "\t  X  -1  -1   0  -1  -1   X   0   X   X \n",
      "\t  X  -1   0   0   0   X   0  -1  -1   X \n",
      "\t  X   X   X   0   0   X   0   X   X   X \n",
      "\t  X   X  -1   X   0  -1  -1   0   0   X \n",
      "\t  X   X   X   X   X   X   X   X 100   X \n",
      "current step: 1 \t score: 0.0\n",
      "\n",
      "current state: [1, 3] -> action: [2, 3] \n",
      "\t  X   X   X   #   X   X   X   X   X   X \n",
      "\t  X  -1  -1   0   X   X   0  -1  -1   X \n",
      "\t  X   0  -1   @  -1  -1   X   0   0   X \n",
      "\t  X  -1   0   X  -1   0   0   X   X   X \n",
      "\t  X  -1   0   0  -1   0   0   0  -1   X \n",
      "\t  X  -1  -1   0  -1  -1   X   0   X   X \n",
      "\t  X  -1   0   0   0   X   0  -1  -1   X \n",
      "\t  X   X   X   0   0   X   0   X   X   X \n",
      "\t  X   X  -1   X   0  -1  -1   0   0   X \n",
      "\t  X   X   X   X   X   X   X   X 100   X \n",
      "current step: 2 \t score: -1.0\n",
      "\n",
      "current state: [2, 3] -> action: [2, 2] \n",
      "\t  X   X   X   #   X   X   X   X   X   X \n",
      "\t  X  -1  -1   0   X   X   0  -1  -1   X \n",
      "\t  X   0   @  -1  -1  -1   X   0   0   X \n",
      "\t  X  -1   0   X  -1   0   0   X   X   X \n",
      "\t  X  -1   0   0  -1   0   0   0  -1   X \n",
      "\t  X  -1  -1   0  -1  -1   X   0   X   X \n",
      "\t  X  -1   0   0   0   X   0  -1  -1   X \n",
      "\t  X   X   X   0   0   X   0   X   X   X \n",
      "\t  X   X  -1   X   0  -1  -1   0   0   X \n",
      "\t  X   X   X   X   X   X   X   X 100   X \n",
      "current step: 3 \t score: -2.0\n",
      "\n",
      "current state: [2, 2] -> action: [3, 2] \n",
      "\t  X   X   X   #   X   X   X   X   X   X \n",
      "\t  X  -1  -1   0   X   X   0  -1  -1   X \n",
      "\t  X   0  -1  -1  -1  -1   X   0   0   X \n",
      "\t  X  -1   @   X  -1   0   0   X   X   X \n",
      "\t  X  -1   0   0  -1   0   0   0  -1   X \n",
      "\t  X  -1  -1   0  -1  -1   X   0   X   X \n",
      "\t  X  -1   0   0   0   X   0  -1  -1   X \n",
      "\t  X   X   X   0   0   X   0   X   X   X \n",
      "\t  X   X  -1   X   0  -1  -1   0   0   X \n",
      "\t  X   X   X   X   X   X   X   X 100   X \n",
      "current step: 4 \t score: -2.0\n",
      "\n",
      "current state: [3, 2] -> action: [4, 2] \n",
      "\t  X   X   X   #   X   X   X   X   X   X \n",
      "\t  X  -1  -1   0   X   X   0  -1  -1   X \n",
      "\t  X   0  -1  -1  -1  -1   X   0   0   X \n",
      "\t  X  -1   0   X  -1   0   0   X   X   X \n",
      "\t  X  -1   @   0  -1   0   0   0  -1   X \n",
      "\t  X  -1  -1   0  -1  -1   X   0   X   X \n",
      "\t  X  -1   0   0   0   X   0  -1  -1   X \n",
      "\t  X   X   X   0   0   X   0   X   X   X \n",
      "\t  X   X  -1   X   0  -1  -1   0   0   X \n",
      "\t  X   X   X   X   X   X   X   X 100   X \n",
      "current step: 5 \t score: -2.0\n",
      "\n",
      "current state: [4, 2] -> action: [4, 3] \n",
      "\t  X   X   X   #   X   X   X   X   X   X \n",
      "\t  X  -1  -1   0   X   X   0  -1  -1   X \n",
      "\t  X   0  -1  -1  -1  -1   X   0   0   X \n",
      "\t  X  -1   0   X  -1   0   0   X   X   X \n",
      "\t  X  -1   0   @  -1   0   0   0  -1   X \n",
      "\t  X  -1  -1   0  -1  -1   X   0   X   X \n",
      "\t  X  -1   0   0   0   X   0  -1  -1   X \n",
      "\t  X   X   X   0   0   X   0   X   X   X \n",
      "\t  X   X  -1   X   0  -1  -1   0   0   X \n",
      "\t  X   X   X   X   X   X   X   X 100   X \n",
      "current step: 6 \t score: -2.0\n",
      "\n",
      "current state: [4, 3] -> action: [4, 2] \n",
      "\t  X   X   X   #   X   X   X   X   X   X \n",
      "\t  X  -1  -1   0   X   X   0  -1  -1   X \n",
      "\t  X   0  -1  -1  -1  -1   X   0   0   X \n",
      "\t  X  -1   0   X  -1   0   0   X   X   X \n",
      "\t  X  -1   @   0  -1   0   0   0  -1   X \n",
      "\t  X  -1  -1   0  -1  -1   X   0   X   X \n",
      "\t  X  -1   0   0   0   X   0  -1  -1   X \n",
      "\t  X   X   X   0   0   X   0   X   X   X \n",
      "\t  X   X  -1   X   0  -1  -1   0   0   X \n",
      "\t  X   X   X   X   X   X   X   X 100   X \n",
      "current step: 7 \t score: -2.0\n",
      "\n",
      "current state: [4, 2] -> action: [3, 2] \n",
      "\t  X   X   X   #   X   X   X   X   X   X \n",
      "\t  X  -1  -1   0   X   X   0  -1  -1   X \n",
      "\t  X   0  -1  -1  -1  -1   X   0   0   X \n",
      "\t  X  -1   @   X  -1   0   0   X   X   X \n",
      "\t  X  -1   0   0  -1   0   0   0  -1   X \n",
      "\t  X  -1  -1   0  -1  -1   X   0   X   X \n",
      "\t  X  -1   0   0   0   X   0  -1  -1   X \n",
      "\t  X   X   X   0   0   X   0   X   X   X \n",
      "\t  X   X  -1   X   0  -1  -1   0   0   X \n",
      "\t  X   X   X   X   X   X   X   X 100   X \n",
      "current step: 8 \t score: -2.0\n",
      "\n",
      "current state: [3, 2] -> action: [4, 2] \n",
      "\t  X   X   X   #   X   X   X   X   X   X \n",
      "\t  X  -1  -1   0   X   X   0  -1  -1   X \n",
      "\t  X   0  -1  -1  -1  -1   X   0   0   X \n",
      "\t  X  -1   0   X  -1   0   0   X   X   X \n",
      "\t  X  -1   @   0  -1   0   0   0  -1   X \n",
      "\t  X  -1  -1   0  -1  -1   X   0   X   X \n",
      "\t  X  -1   0   0   0   X   0  -1  -1   X \n",
      "\t  X   X   X   0   0   X   0   X   X   X \n",
      "\t  X   X  -1   X   0  -1  -1   0   0   X \n",
      "\t  X   X   X   X   X   X   X   X 100   X \n",
      "current step: 9 \t score: -2.0\n",
      "\n",
      "current state: [4, 2] -> action: [4, 3] \n",
      "\t  X   X   X   #   X   X   X   X   X   X \n",
      "\t  X  -1  -1   0   X   X   0  -1  -1   X \n",
      "\t  X   0  -1  -1  -1  -1   X   0   0   X \n",
      "\t  X  -1   0   X  -1   0   0   X   X   X \n",
      "\t  X  -1   0   @  -1   0   0   0  -1   X \n",
      "\t  X  -1  -1   0  -1  -1   X   0   X   X \n",
      "\t  X  -1   0   0   0   X   0  -1  -1   X \n",
      "\t  X   X   X   0   0   X   0   X   X   X \n",
      "\t  X   X  -1   X   0  -1  -1   0   0   X \n",
      "\t  X   X   X   X   X   X   X   X 100   X \n",
      "current step: 10 \t score: -2.0\n",
      "\n",
      "current state: [4, 3] -> action: [5, 3] \n",
      "\t  X   X   X   #   X   X   X   X   X   X \n",
      "\t  X  -1  -1   0   X   X   0  -1  -1   X \n",
      "\t  X   0  -1  -1  -1  -1   X   0   0   X \n",
      "\t  X  -1   0   X  -1   0   0   X   X   X \n",
      "\t  X  -1   0   0  -1   0   0   0  -1   X \n",
      "\t  X  -1  -1   @  -1  -1   X   0   X   X \n",
      "\t  X  -1   0   0   0   X   0  -1  -1   X \n",
      "\t  X   X   X   0   0   X   0   X   X   X \n",
      "\t  X   X  -1   X   0  -1  -1   0   0   X \n",
      "\t  X   X   X   X   X   X   X   X 100   X \n",
      "current step: 11 \t score: -2.0\n",
      "\n",
      "current state: [5, 3] -> action: [6, 3] \n",
      "\t  X   X   X   #   X   X   X   X   X   X \n",
      "\t  X  -1  -1   0   X   X   0  -1  -1   X \n",
      "\t  X   0  -1  -1  -1  -1   X   0   0   X \n",
      "\t  X  -1   0   X  -1   0   0   X   X   X \n",
      "\t  X  -1   0   0  -1   0   0   0  -1   X \n",
      "\t  X  -1  -1   0  -1  -1   X   0   X   X \n",
      "\t  X  -1   0   @   0   X   0  -1  -1   X \n",
      "\t  X   X   X   0   0   X   0   X   X   X \n",
      "\t  X   X  -1   X   0  -1  -1   0   0   X \n",
      "\t  X   X   X   X   X   X   X   X 100   X \n",
      "current step: 12 \t score: -2.0\n",
      "\n",
      "current state: [6, 3] -> action: [7, 3] \n",
      "\t  X   X   X   #   X   X   X   X   X   X \n",
      "\t  X  -1  -1   0   X   X   0  -1  -1   X \n",
      "\t  X   0  -1  -1  -1  -1   X   0   0   X \n",
      "\t  X  -1   0   X  -1   0   0   X   X   X \n",
      "\t  X  -1   0   0  -1   0   0   0  -1   X \n",
      "\t  X  -1  -1   0  -1  -1   X   0   X   X \n",
      "\t  X  -1   0   0   0   X   0  -1  -1   X \n",
      "\t  X   X   X   @   0   X   0   X   X   X \n",
      "\t  X   X  -1   X   0  -1  -1   0   0   X \n",
      "\t  X   X   X   X   X   X   X   X 100   X \n",
      "current step: 13 \t score: -2.0\n",
      "\n",
      "current state: [7, 3] -> action: [6, 3] \n",
      "\t  X   X   X   #   X   X   X   X   X   X \n",
      "\t  X  -1  -1   0   X   X   0  -1  -1   X \n",
      "\t  X   0  -1  -1  -1  -1   X   0   0   X \n",
      "\t  X  -1   0   X  -1   0   0   X   X   X \n",
      "\t  X  -1   0   0  -1   0   0   0  -1   X \n",
      "\t  X  -1  -1   0  -1  -1   X   0   X   X \n",
      "\t  X  -1   0   @   0   X   0  -1  -1   X \n",
      "\t  X   X   X   0   0   X   0   X   X   X \n",
      "\t  X   X  -1   X   0  -1  -1   0   0   X \n",
      "\t  X   X   X   X   X   X   X   X 100   X \n",
      "current step: 14 \t score: -2.0\n",
      "\n",
      "current state: [6, 3] -> action: [5, 3] \n",
      "\t  X   X   X   #   X   X   X   X   X   X \n",
      "\t  X  -1  -1   0   X   X   0  -1  -1   X \n",
      "\t  X   0  -1  -1  -1  -1   X   0   0   X \n",
      "\t  X  -1   0   X  -1   0   0   X   X   X \n",
      "\t  X  -1   0   0  -1   0   0   0  -1   X \n",
      "\t  X  -1  -1   @  -1  -1   X   0   X   X \n",
      "\t  X  -1   0   0   0   X   0  -1  -1   X \n",
      "\t  X   X   X   0   0   X   0   X   X   X \n",
      "\t  X   X  -1   X   0  -1  -1   0   0   X \n",
      "\t  X   X   X   X   X   X   X   X 100   X \n",
      "current step: 15 \t score: -2.0\n",
      "\n",
      "current state: [5, 3] -> action: [4, 3] \n",
      "\t  X   X   X   #   X   X   X   X   X   X \n",
      "\t  X  -1  -1   0   X   X   0  -1  -1   X \n",
      "\t  X   0  -1  -1  -1  -1   X   0   0   X \n",
      "\t  X  -1   0   X  -1   0   0   X   X   X \n",
      "\t  X  -1   0   @  -1   0   0   0  -1   X \n",
      "\t  X  -1  -1   0  -1  -1   X   0   X   X \n",
      "\t  X  -1   0   0   0   X   0  -1  -1   X \n",
      "\t  X   X   X   0   0   X   0   X   X   X \n",
      "\t  X   X  -1   X   0  -1  -1   0   0   X \n",
      "\t  X   X   X   X   X   X   X   X 100   X \n",
      "current step: 16 \t score: -2.0\n",
      "\n",
      "current state: [4, 3] -> action: [4, 2] \n",
      "\t  X   X   X   #   X   X   X   X   X   X \n",
      "\t  X  -1  -1   0   X   X   0  -1  -1   X \n",
      "\t  X   0  -1  -1  -1  -1   X   0   0   X \n",
      "\t  X  -1   0   X  -1   0   0   X   X   X \n",
      "\t  X  -1   @   0  -1   0   0   0  -1   X \n",
      "\t  X  -1  -1   0  -1  -1   X   0   X   X \n",
      "\t  X  -1   0   0   0   X   0  -1  -1   X \n",
      "\t  X   X   X   0   0   X   0   X   X   X \n",
      "\t  X   X  -1   X   0  -1  -1   0   0   X \n",
      "\t  X   X   X   X   X   X   X   X 100   X \n",
      "current step: 17 \t score: -2.0\n",
      "\n",
      "current state: [4, 2] -> action: [3, 2] \n",
      "\t  X   X   X   #   X   X   X   X   X   X \n",
      "\t  X  -1  -1   0   X   X   0  -1  -1   X \n",
      "\t  X   0  -1  -1  -1  -1   X   0   0   X \n",
      "\t  X  -1   @   X  -1   0   0   X   X   X \n",
      "\t  X  -1   0   0  -1   0   0   0  -1   X \n",
      "\t  X  -1  -1   0  -1  -1   X   0   X   X \n",
      "\t  X  -1   0   0   0   X   0  -1  -1   X \n",
      "\t  X   X   X   0   0   X   0   X   X   X \n",
      "\t  X   X  -1   X   0  -1  -1   0   0   X \n",
      "\t  X   X   X   X   X   X   X   X 100   X \n",
      "current step: 18 \t score: -2.0\n",
      "\n",
      "current state: [3, 2] -> action: [4, 2] \n",
      "\t  X   X   X   #   X   X   X   X   X   X \n",
      "\t  X  -1  -1   0   X   X   0  -1  -1   X \n",
      "\t  X   0  -1  -1  -1  -1   X   0   0   X \n",
      "\t  X  -1   0   X  -1   0   0   X   X   X \n",
      "\t  X  -1   @   0  -1   0   0   0  -1   X \n",
      "\t  X  -1  -1   0  -1  -1   X   0   X   X \n",
      "\t  X  -1   0   0   0   X   0  -1  -1   X \n",
      "\t  X   X   X   0   0   X   0   X   X   X \n",
      "\t  X   X  -1   X   0  -1  -1   0   0   X \n",
      "\t  X   X   X   X   X   X   X   X 100   X \n",
      "current step: 19 \t score: -2.0\n",
      "\n",
      "current state: [4, 2] -> action: [3, 2] \n",
      "\t  X   X   X   #   X   X   X   X   X   X \n",
      "\t  X  -1  -1   0   X   X   0  -1  -1   X \n",
      "\t  X   0  -1  -1  -1  -1   X   0   0   X \n",
      "\t  X  -1   @   X  -1   0   0   X   X   X \n",
      "\t  X  -1   0   0  -1   0   0   0  -1   X \n",
      "\t  X  -1  -1   0  -1  -1   X   0   X   X \n",
      "\t  X  -1   0   0   0   X   0  -1  -1   X \n",
      "\t  X   X   X   0   0   X   0   X   X   X \n",
      "\t  X   X  -1   X   0  -1  -1   0   0   X \n",
      "\t  X   X   X   X   X   X   X   X 100   X \n",
      "current step: 20 \t score: -2.0\n",
      "\n",
      "current state: [3, 2] -> action: [4, 2] \n",
      "\t  X   X   X   #   X   X   X   X   X   X \n",
      "\t  X  -1  -1   0   X   X   0  -1  -1   X \n",
      "\t  X   0  -1  -1  -1  -1   X   0   0   X \n",
      "\t  X  -1   0   X  -1   0   0   X   X   X \n",
      "\t  X  -1   @   0  -1   0   0   0  -1   X \n",
      "\t  X  -1  -1   0  -1  -1   X   0   X   X \n",
      "\t  X  -1   0   0   0   X   0  -1  -1   X \n",
      "\t  X   X   X   0   0   X   0   X   X   X \n",
      "\t  X   X  -1   X   0  -1  -1   0   0   X \n",
      "\t  X   X   X   X   X   X   X   X 100   X \n",
      "current step: 21 \t score: -2.0\n",
      "\n",
      "current state: [4, 2] -> action: [4, 3] \n",
      "\t  X   X   X   #   X   X   X   X   X   X \n",
      "\t  X  -1  -1   0   X   X   0  -1  -1   X \n",
      "\t  X   0  -1  -1  -1  -1   X   0   0   X \n",
      "\t  X  -1   0   X  -1   0   0   X   X   X \n",
      "\t  X  -1   0   @  -1   0   0   0  -1   X \n",
      "\t  X  -1  -1   0  -1  -1   X   0   X   X \n",
      "\t  X  -1   0   0   0   X   0  -1  -1   X \n",
      "\t  X   X   X   0   0   X   0   X   X   X \n",
      "\t  X   X  -1   X   0  -1  -1   0   0   X \n",
      "\t  X   X   X   X   X   X   X   X 100   X \n",
      "current step: 22 \t score: -2.0\n",
      "\n",
      "current state: [4, 3] -> action: [4, 2] \n",
      "\t  X   X   X   #   X   X   X   X   X   X \n",
      "\t  X  -1  -1   0   X   X   0  -1  -1   X \n",
      "\t  X   0  -1  -1  -1  -1   X   0   0   X \n",
      "\t  X  -1   0   X  -1   0   0   X   X   X \n",
      "\t  X  -1   @   0  -1   0   0   0  -1   X \n",
      "\t  X  -1  -1   0  -1  -1   X   0   X   X \n",
      "\t  X  -1   0   0   0   X   0  -1  -1   X \n",
      "\t  X   X   X   0   0   X   0   X   X   X \n",
      "\t  X   X  -1   X   0  -1  -1   0   0   X \n",
      "\t  X   X   X   X   X   X   X   X 100   X \n",
      "current step: 23 \t score: -2.0\n",
      "\n",
      "current state: [4, 2] -> action: [3, 2] \n",
      "\t  X   X   X   #   X   X   X   X   X   X \n",
      "\t  X  -1  -1   0   X   X   0  -1  -1   X \n",
      "\t  X   0  -1  -1  -1  -1   X   0   0   X \n",
      "\t  X  -1   @   X  -1   0   0   X   X   X \n",
      "\t  X  -1   0   0  -1   0   0   0  -1   X \n",
      "\t  X  -1  -1   0  -1  -1   X   0   X   X \n",
      "\t  X  -1   0   0   0   X   0  -1  -1   X \n",
      "\t  X   X   X   0   0   X   0   X   X   X \n",
      "\t  X   X  -1   X   0  -1  -1   0   0   X \n",
      "\t  X   X   X   X   X   X   X   X 100   X \n",
      "current step: 24 \t score: -2.0\n",
      "\n",
      "current state: [3, 2] -> action: [4, 2] \n",
      "\t  X   X   X   #   X   X   X   X   X   X \n",
      "\t  X  -1  -1   0   X   X   0  -1  -1   X \n",
      "\t  X   0  -1  -1  -1  -1   X   0   0   X \n",
      "\t  X  -1   0   X  -1   0   0   X   X   X \n",
      "\t  X  -1   @   0  -1   0   0   0  -1   X \n",
      "\t  X  -1  -1   0  -1  -1   X   0   X   X \n",
      "\t  X  -1   0   0   0   X   0  -1  -1   X \n",
      "\t  X   X   X   0   0   X   0   X   X   X \n",
      "\t  X   X  -1   X   0  -1  -1   0   0   X \n",
      "\t  X   X   X   X   X   X   X   X 100   X \n",
      "current step: 25 \t score: -2.0\n",
      "\n",
      "current state: [4, 2] -> action: [3, 2] \n",
      "\t  X   X   X   #   X   X   X   X   X   X \n",
      "\t  X  -1  -1   0   X   X   0  -1  -1   X \n",
      "\t  X   0  -1  -1  -1  -1   X   0   0   X \n",
      "\t  X  -1   @   X  -1   0   0   X   X   X \n",
      "\t  X  -1   0   0  -1   0   0   0  -1   X \n",
      "\t  X  -1  -1   0  -1  -1   X   0   X   X \n",
      "\t  X  -1   0   0   0   X   0  -1  -1   X \n",
      "\t  X   X   X   0   0   X   0   X   X   X \n",
      "\t  X   X  -1   X   0  -1  -1   0   0   X \n",
      "\t  X   X   X   X   X   X   X   X 100   X \n",
      "current step: 26 \t score: -2.0\n",
      "\n",
      "current state: [3, 2] -> action: [4, 2] \n",
      "\t  X   X   X   #   X   X   X   X   X   X \n",
      "\t  X  -1  -1   0   X   X   0  -1  -1   X \n",
      "\t  X   0  -1  -1  -1  -1   X   0   0   X \n",
      "\t  X  -1   0   X  -1   0   0   X   X   X \n",
      "\t  X  -1   @   0  -1   0   0   0  -1   X \n",
      "\t  X  -1  -1   0  -1  -1   X   0   X   X \n",
      "\t  X  -1   0   0   0   X   0  -1  -1   X \n",
      "\t  X   X   X   0   0   X   0   X   X   X \n",
      "\t  X   X  -1   X   0  -1  -1   0   0   X \n",
      "\t  X   X   X   X   X   X   X   X 100   X \n",
      "current step: 27 \t score: -2.0\n",
      "\n",
      "current state: [4, 2] -> action: [4, 3] \n",
      "\t  X   X   X   #   X   X   X   X   X   X \n",
      "\t  X  -1  -1   0   X   X   0  -1  -1   X \n",
      "\t  X   0  -1  -1  -1  -1   X   0   0   X \n",
      "\t  X  -1   0   X  -1   0   0   X   X   X \n",
      "\t  X  -1   0   @  -1   0   0   0  -1   X \n",
      "\t  X  -1  -1   0  -1  -1   X   0   X   X \n",
      "\t  X  -1   0   0   0   X   0  -1  -1   X \n",
      "\t  X   X   X   0   0   X   0   X   X   X \n",
      "\t  X   X  -1   X   0  -1  -1   0   0   X \n",
      "\t  X   X   X   X   X   X   X   X 100   X \n",
      "current step: 28 \t score: -2.0\n",
      "\n",
      "current state: [4, 3] -> action: [5, 3] \n",
      "\t  X   X   X   #   X   X   X   X   X   X \n",
      "\t  X  -1  -1   0   X   X   0  -1  -1   X \n",
      "\t  X   0  -1  -1  -1  -1   X   0   0   X \n",
      "\t  X  -1   0   X  -1   0   0   X   X   X \n",
      "\t  X  -1   0   0  -1   0   0   0  -1   X \n",
      "\t  X  -1  -1   @  -1  -1   X   0   X   X \n",
      "\t  X  -1   0   0   0   X   0  -1  -1   X \n",
      "\t  X   X   X   0   0   X   0   X   X   X \n",
      "\t  X   X  -1   X   0  -1  -1   0   0   X \n",
      "\t  X   X   X   X   X   X   X   X 100   X \n",
      "current step: 29 \t score: -2.0\n",
      "\n",
      "current state: [5, 3] -> action: [4, 3] \n",
      "\t  X   X   X   #   X   X   X   X   X   X \n",
      "\t  X  -1  -1   0   X   X   0  -1  -1   X \n",
      "\t  X   0  -1  -1  -1  -1   X   0   0   X \n",
      "\t  X  -1   0   X  -1   0   0   X   X   X \n",
      "\t  X  -1   0   @  -1   0   0   0  -1   X \n",
      "\t  X  -1  -1   0  -1  -1   X   0   X   X \n",
      "\t  X  -1   0   0   0   X   0  -1  -1   X \n",
      "\t  X   X   X   0   0   X   0   X   X   X \n",
      "\t  X   X  -1   X   0  -1  -1   0   0   X \n",
      "\t  X   X   X   X   X   X   X   X 100   X \n",
      "current step: 30 \t score: -2.0\n",
      "\n",
      "current state: [4, 3] -> action: [4, 2] \n",
      "\t  X   X   X   #   X   X   X   X   X   X \n",
      "\t  X  -1  -1   0   X   X   0  -1  -1   X \n",
      "\t  X   0  -1  -1  -1  -1   X   0   0   X \n",
      "\t  X  -1   0   X  -1   0   0   X   X   X \n",
      "\t  X  -1   @   0  -1   0   0   0  -1   X \n",
      "\t  X  -1  -1   0  -1  -1   X   0   X   X \n",
      "\t  X  -1   0   0   0   X   0  -1  -1   X \n",
      "\t  X   X   X   0   0   X   0   X   X   X \n",
      "\t  X   X  -1   X   0  -1  -1   0   0   X \n",
      "\t  X   X   X   X   X   X   X   X 100   X \n",
      "current step: 31 \t score: -2.0\n",
      "\n",
      "current state: [4, 2] -> action: [4, 3] \n",
      "\t  X   X   X   #   X   X   X   X   X   X \n",
      "\t  X  -1  -1   0   X   X   0  -1  -1   X \n",
      "\t  X   0  -1  -1  -1  -1   X   0   0   X \n",
      "\t  X  -1   0   X  -1   0   0   X   X   X \n",
      "\t  X  -1   0   @  -1   0   0   0  -1   X \n",
      "\t  X  -1  -1   0  -1  -1   X   0   X   X \n",
      "\t  X  -1   0   0   0   X   0  -1  -1   X \n",
      "\t  X   X   X   0   0   X   0   X   X   X \n",
      "\t  X   X  -1   X   0  -1  -1   0   0   X \n",
      "\t  X   X   X   X   X   X   X   X 100   X \n",
      "current step: 32 \t score: -2.0\n",
      "\n",
      "current state: [4, 3] -> action: [5, 3] \n",
      "\t  X   X   X   #   X   X   X   X   X   X \n",
      "\t  X  -1  -1   0   X   X   0  -1  -1   X \n",
      "\t  X   0  -1  -1  -1  -1   X   0   0   X \n",
      "\t  X  -1   0   X  -1   0   0   X   X   X \n",
      "\t  X  -1   0   0  -1   0   0   0  -1   X \n",
      "\t  X  -1  -1   @  -1  -1   X   0   X   X \n",
      "\t  X  -1   0   0   0   X   0  -1  -1   X \n",
      "\t  X   X   X   0   0   X   0   X   X   X \n",
      "\t  X   X  -1   X   0  -1  -1   0   0   X \n",
      "\t  X   X   X   X   X   X   X   X 100   X \n",
      "current step: 33 \t score: -2.0\n",
      "\n",
      "current state: [5, 3] -> action: [6, 3] \n",
      "\t  X   X   X   #   X   X   X   X   X   X \n",
      "\t  X  -1  -1   0   X   X   0  -1  -1   X \n",
      "\t  X   0  -1  -1  -1  -1   X   0   0   X \n",
      "\t  X  -1   0   X  -1   0   0   X   X   X \n",
      "\t  X  -1   0   0  -1   0   0   0  -1   X \n",
      "\t  X  -1  -1   0  -1  -1   X   0   X   X \n",
      "\t  X  -1   0   @   0   X   0  -1  -1   X \n",
      "\t  X   X   X   0   0   X   0   X   X   X \n",
      "\t  X   X  -1   X   0  -1  -1   0   0   X \n",
      "\t  X   X   X   X   X   X   X   X 100   X \n",
      "current step: 34 \t score: -2.0\n",
      "\n",
      "current state: [6, 3] -> action: [5, 3] \n",
      "\t  X   X   X   #   X   X   X   X   X   X \n",
      "\t  X  -1  -1   0   X   X   0  -1  -1   X \n",
      "\t  X   0  -1  -1  -1  -1   X   0   0   X \n",
      "\t  X  -1   0   X  -1   0   0   X   X   X \n",
      "\t  X  -1   0   0  -1   0   0   0  -1   X \n",
      "\t  X  -1  -1   @  -1  -1   X   0   X   X \n",
      "\t  X  -1   0   0   0   X   0  -1  -1   X \n",
      "\t  X   X   X   0   0   X   0   X   X   X \n",
      "\t  X   X  -1   X   0  -1  -1   0   0   X \n",
      "\t  X   X   X   X   X   X   X   X 100   X \n",
      "current step: 35 \t score: -2.0\n",
      "\n",
      "current state: [5, 3] -> action: [4, 3] \n",
      "\t  X   X   X   #   X   X   X   X   X   X \n",
      "\t  X  -1  -1   0   X   X   0  -1  -1   X \n",
      "\t  X   0  -1  -1  -1  -1   X   0   0   X \n",
      "\t  X  -1   0   X  -1   0   0   X   X   X \n",
      "\t  X  -1   0   @  -1   0   0   0  -1   X \n",
      "\t  X  -1  -1   0  -1  -1   X   0   X   X \n",
      "\t  X  -1   0   0   0   X   0  -1  -1   X \n",
      "\t  X   X   X   0   0   X   0   X   X   X \n",
      "\t  X   X  -1   X   0  -1  -1   0   0   X \n",
      "\t  X   X   X   X   X   X   X   X 100   X \n",
      "current step: 36 \t score: -2.0\n",
      "\n",
      "current state: [4, 3] -> action: [5, 3] \n",
      "\t  X   X   X   #   X   X   X   X   X   X \n",
      "\t  X  -1  -1   0   X   X   0  -1  -1   X \n",
      "\t  X   0  -1  -1  -1  -1   X   0   0   X \n",
      "\t  X  -1   0   X  -1   0   0   X   X   X \n",
      "\t  X  -1   0   0  -1   0   0   0  -1   X \n",
      "\t  X  -1  -1   @  -1  -1   X   0   X   X \n",
      "\t  X  -1   0   0   0   X   0  -1  -1   X \n",
      "\t  X   X   X   0   0   X   0   X   X   X \n",
      "\t  X   X  -1   X   0  -1  -1   0   0   X \n",
      "\t  X   X   X   X   X   X   X   X 100   X \n",
      "current step: 37 \t score: -2.0\n",
      "\n",
      "current state: [5, 3] -> action: [6, 3] \n",
      "\t  X   X   X   #   X   X   X   X   X   X \n",
      "\t  X  -1  -1   0   X   X   0  -1  -1   X \n",
      "\t  X   0  -1  -1  -1  -1   X   0   0   X \n",
      "\t  X  -1   0   X  -1   0   0   X   X   X \n",
      "\t  X  -1   0   0  -1   0   0   0  -1   X \n",
      "\t  X  -1  -1   0  -1  -1   X   0   X   X \n",
      "\t  X  -1   0   @   0   X   0  -1  -1   X \n",
      "\t  X   X   X   0   0   X   0   X   X   X \n",
      "\t  X   X  -1   X   0  -1  -1   0   0   X \n",
      "\t  X   X   X   X   X   X   X   X 100   X \n",
      "current step: 38 \t score: -2.0\n",
      "\n",
      "current state: [6, 3] -> action: [6, 2] \n",
      "\t  X   X   X   #   X   X   X   X   X   X \n",
      "\t  X  -1  -1   0   X   X   0  -1  -1   X \n",
      "\t  X   0  -1  -1  -1  -1   X   0   0   X \n",
      "\t  X  -1   0   X  -1   0   0   X   X   X \n",
      "\t  X  -1   0   0  -1   0   0   0  -1   X \n",
      "\t  X  -1  -1   0  -1  -1   X   0   X   X \n",
      "\t  X  -1   @   0   0   X   0  -1  -1   X \n",
      "\t  X   X   X   0   0   X   0   X   X   X \n",
      "\t  X   X  -1   X   0  -1  -1   0   0   X \n",
      "\t  X   X   X   X   X   X   X   X 100   X \n",
      "current step: 39 \t score: -2.0\n",
      "\n",
      "current state: [6, 2] -> action: [6, 3] \n",
      "\t  X   X   X   #   X   X   X   X   X   X \n",
      "\t  X  -1  -1   0   X   X   0  -1  -1   X \n",
      "\t  X   0  -1  -1  -1  -1   X   0   0   X \n",
      "\t  X  -1   0   X  -1   0   0   X   X   X \n",
      "\t  X  -1   0   0  -1   0   0   0  -1   X \n",
      "\t  X  -1  -1   0  -1  -1   X   0   X   X \n",
      "\t  X  -1   0   @   0   X   0  -1  -1   X \n",
      "\t  X   X   X   0   0   X   0   X   X   X \n",
      "\t  X   X  -1   X   0  -1  -1   0   0   X \n",
      "\t  X   X   X   X   X   X   X   X 100   X \n",
      "current step: 40 \t score: -2.0\n",
      "\n",
      "current state: [6, 3] -> action: [5, 3] \n",
      "\t  X   X   X   #   X   X   X   X   X   X \n",
      "\t  X  -1  -1   0   X   X   0  -1  -1   X \n",
      "\t  X   0  -1  -1  -1  -1   X   0   0   X \n",
      "\t  X  -1   0   X  -1   0   0   X   X   X \n",
      "\t  X  -1   0   0  -1   0   0   0  -1   X \n",
      "\t  X  -1  -1   @  -1  -1   X   0   X   X \n",
      "\t  X  -1   0   0   0   X   0  -1  -1   X \n",
      "\t  X   X   X   0   0   X   0   X   X   X \n",
      "\t  X   X  -1   X   0  -1  -1   0   0   X \n",
      "\t  X   X   X   X   X   X   X   X 100   X \n",
      "current step: 41 \t score: -2.0\n",
      "\n",
      "current state: [5, 3] -> action: [6, 3] \n",
      "\t  X   X   X   #   X   X   X   X   X   X \n",
      "\t  X  -1  -1   0   X   X   0  -1  -1   X \n",
      "\t  X   0  -1  -1  -1  -1   X   0   0   X \n",
      "\t  X  -1   0   X  -1   0   0   X   X   X \n",
      "\t  X  -1   0   0  -1   0   0   0  -1   X \n",
      "\t  X  -1  -1   0  -1  -1   X   0   X   X \n",
      "\t  X  -1   0   @   0   X   0  -1  -1   X \n",
      "\t  X   X   X   0   0   X   0   X   X   X \n",
      "\t  X   X  -1   X   0  -1  -1   0   0   X \n",
      "\t  X   X   X   X   X   X   X   X 100   X \n",
      "current step: 42 \t score: -2.0\n",
      "\n",
      "current state: [6, 3] -> action: [6, 2] \n",
      "\t  X   X   X   #   X   X   X   X   X   X \n",
      "\t  X  -1  -1   0   X   X   0  -1  -1   X \n",
      "\t  X   0  -1  -1  -1  -1   X   0   0   X \n",
      "\t  X  -1   0   X  -1   0   0   X   X   X \n",
      "\t  X  -1   0   0  -1   0   0   0  -1   X \n",
      "\t  X  -1  -1   0  -1  -1   X   0   X   X \n",
      "\t  X  -1   @   0   0   X   0  -1  -1   X \n",
      "\t  X   X   X   0   0   X   0   X   X   X \n",
      "\t  X   X  -1   X   0  -1  -1   0   0   X \n",
      "\t  X   X   X   X   X   X   X   X 100   X \n",
      "current step: 43 \t score: -2.0\n",
      "\n",
      "current state: [6, 2] -> action: [6, 3] \n",
      "\t  X   X   X   #   X   X   X   X   X   X \n",
      "\t  X  -1  -1   0   X   X   0  -1  -1   X \n",
      "\t  X   0  -1  -1  -1  -1   X   0   0   X \n",
      "\t  X  -1   0   X  -1   0   0   X   X   X \n",
      "\t  X  -1   0   0  -1   0   0   0  -1   X \n",
      "\t  X  -1  -1   0  -1  -1   X   0   X   X \n",
      "\t  X  -1   0   @   0   X   0  -1  -1   X \n",
      "\t  X   X   X   0   0   X   0   X   X   X \n",
      "\t  X   X  -1   X   0  -1  -1   0   0   X \n",
      "\t  X   X   X   X   X   X   X   X 100   X \n",
      "current step: 44 \t score: -2.0\n",
      "\n",
      "current state: [6, 3] -> action: [6, 2] \n",
      "\t  X   X   X   #   X   X   X   X   X   X \n",
      "\t  X  -1  -1   0   X   X   0  -1  -1   X \n",
      "\t  X   0  -1  -1  -1  -1   X   0   0   X \n",
      "\t  X  -1   0   X  -1   0   0   X   X   X \n",
      "\t  X  -1   0   0  -1   0   0   0  -1   X \n",
      "\t  X  -1  -1   0  -1  -1   X   0   X   X \n",
      "\t  X  -1   @   0   0   X   0  -1  -1   X \n",
      "\t  X   X   X   0   0   X   0   X   X   X \n",
      "\t  X   X  -1   X   0  -1  -1   0   0   X \n",
      "\t  X   X   X   X   X   X   X   X 100   X \n",
      "current step: 45 \t score: -2.0\n",
      "\n",
      "current state: [6, 2] -> action: [6, 3] \n",
      "\t  X   X   X   #   X   X   X   X   X   X \n",
      "\t  X  -1  -1   0   X   X   0  -1  -1   X \n",
      "\t  X   0  -1  -1  -1  -1   X   0   0   X \n",
      "\t  X  -1   0   X  -1   0   0   X   X   X \n",
      "\t  X  -1   0   0  -1   0   0   0  -1   X \n",
      "\t  X  -1  -1   0  -1  -1   X   0   X   X \n",
      "\t  X  -1   0   @   0   X   0  -1  -1   X \n",
      "\t  X   X   X   0   0   X   0   X   X   X \n",
      "\t  X   X  -1   X   0  -1  -1   0   0   X \n",
      "\t  X   X   X   X   X   X   X   X 100   X \n",
      "current step: 46 \t score: -2.0\n",
      "\n",
      "current state: [6, 3] -> action: [6, 4] \n",
      "\t  X   X   X   #   X   X   X   X   X   X \n",
      "\t  X  -1  -1   0   X   X   0  -1  -1   X \n",
      "\t  X   0  -1  -1  -1  -1   X   0   0   X \n",
      "\t  X  -1   0   X  -1   0   0   X   X   X \n",
      "\t  X  -1   0   0  -1   0   0   0  -1   X \n",
      "\t  X  -1  -1   0  -1  -1   X   0   X   X \n",
      "\t  X  -1   0   0   @   X   0  -1  -1   X \n",
      "\t  X   X   X   0   0   X   0   X   X   X \n",
      "\t  X   X  -1   X   0  -1  -1   0   0   X \n",
      "\t  X   X   X   X   X   X   X   X 100   X \n",
      "current step: 47 \t score: -2.0\n",
      "\n",
      "current state: [6, 4] -> action: [7, 4] \n",
      "\t  X   X   X   #   X   X   X   X   X   X \n",
      "\t  X  -1  -1   0   X   X   0  -1  -1   X \n",
      "\t  X   0  -1  -1  -1  -1   X   0   0   X \n",
      "\t  X  -1   0   X  -1   0   0   X   X   X \n",
      "\t  X  -1   0   0  -1   0   0   0  -1   X \n",
      "\t  X  -1  -1   0  -1  -1   X   0   X   X \n",
      "\t  X  -1   0   0   0   X   0  -1  -1   X \n",
      "\t  X   X   X   0   @   X   0   X   X   X \n",
      "\t  X   X  -1   X   0  -1  -1   0   0   X \n",
      "\t  X   X   X   X   X   X   X   X 100   X \n",
      "current step: 48 \t score: -2.0\n",
      "\n",
      "current state: [7, 4] -> action: [8, 4] \n",
      "\t  X   X   X   #   X   X   X   X   X   X \n",
      "\t  X  -1  -1   0   X   X   0  -1  -1   X \n",
      "\t  X   0  -1  -1  -1  -1   X   0   0   X \n",
      "\t  X  -1   0   X  -1   0   0   X   X   X \n",
      "\t  X  -1   0   0  -1   0   0   0  -1   X \n",
      "\t  X  -1  -1   0  -1  -1   X   0   X   X \n",
      "\t  X  -1   0   0   0   X   0  -1  -1   X \n",
      "\t  X   X   X   0   0   X   0   X   X   X \n",
      "\t  X   X  -1   X   @  -1  -1   0   0   X \n",
      "\t  X   X   X   X   X   X   X   X 100   X \n",
      "current step: 49 \t score: -2.0\n",
      "\n",
      "current state: [8, 4] -> action: [8, 5] \n",
      "\t  X   X   X   #   X   X   X   X   X   X \n",
      "\t  X  -1  -1   0   X   X   0  -1  -1   X \n",
      "\t  X   0  -1  -1  -1  -1   X   0   0   X \n",
      "\t  X  -1   0   X  -1   0   0   X   X   X \n",
      "\t  X  -1   0   0  -1   0   0   0  -1   X \n",
      "\t  X  -1  -1   0  -1  -1   X   0   X   X \n",
      "\t  X  -1   0   0   0   X   0  -1  -1   X \n",
      "\t  X   X   X   0   0   X   0   X   X   X \n",
      "\t  X   X  -1   X   0   @  -1   0   0   X \n",
      "\t  X   X   X   X   X   X   X   X 100   X \n",
      "current step: 50 \t score: -3.0\n",
      "\n",
      "current state: [8, 5] -> action: [8, 6] \n",
      "\t  X   X   X   #   X   X   X   X   X   X \n",
      "\t  X  -1  -1   0   X   X   0  -1  -1   X \n",
      "\t  X   0  -1  -1  -1  -1   X   0   0   X \n",
      "\t  X  -1   0   X  -1   0   0   X   X   X \n",
      "\t  X  -1   0   0  -1   0   0   0  -1   X \n",
      "\t  X  -1  -1   0  -1  -1   X   0   X   X \n",
      "\t  X  -1   0   0   0   X   0  -1  -1   X \n",
      "\t  X   X   X   0   0   X   0   X   X   X \n",
      "\t  X   X  -1   X   0  -1   @   0   0   X \n",
      "\t  X   X   X   X   X   X   X   X 100   X \n",
      "current step: 51 \t score: -4.0\n",
      "\n",
      "current state: [8, 6] -> action: [8, 7] \n",
      "\t  X   X   X   #   X   X   X   X   X   X \n",
      "\t  X  -1  -1   0   X   X   0  -1  -1   X \n",
      "\t  X   0  -1  -1  -1  -1   X   0   0   X \n",
      "\t  X  -1   0   X  -1   0   0   X   X   X \n",
      "\t  X  -1   0   0  -1   0   0   0  -1   X \n",
      "\t  X  -1  -1   0  -1  -1   X   0   X   X \n",
      "\t  X  -1   0   0   0   X   0  -1  -1   X \n",
      "\t  X   X   X   0   0   X   0   X   X   X \n",
      "\t  X   X  -1   X   0  -1  -1   @   0   X \n",
      "\t  X   X   X   X   X   X   X   X 100   X \n",
      "current step: 52 \t score: -4.0\n",
      "\n",
      "current state: [8, 7] -> action: [8, 8] \n",
      "\t  X   X   X   #   X   X   X   X   X   X \n",
      "\t  X  -1  -1   0   X   X   0  -1  -1   X \n",
      "\t  X   0  -1  -1  -1  -1   X   0   0   X \n",
      "\t  X  -1   0   X  -1   0   0   X   X   X \n",
      "\t  X  -1   0   0  -1   0   0   0  -1   X \n",
      "\t  X  -1  -1   0  -1  -1   X   0   X   X \n",
      "\t  X  -1   0   0   0   X   0  -1  -1   X \n",
      "\t  X   X   X   0   0   X   0   X   X   X \n",
      "\t  X   X  -1   X   0  -1  -1   0   @   X \n",
      "\t  X   X   X   X   X   X   X   X 100   X \n",
      "current step: 53 \t score: -4.0\n",
      "\n",
      "current state: [8, 8] -> action: [9, 8] \n",
      "\t  X   X   X   #   X   X   X   X   X   X \n",
      "\t  X  -1  -1   0   X   X   0  -1  -1   X \n",
      "\t  X   0  -1  -1  -1  -1   X   0   0   X \n",
      "\t  X  -1   0   X  -1   0   0   X   X   X \n",
      "\t  X  -1   0   0  -1   0   0   0  -1   X \n",
      "\t  X  -1  -1   0  -1  -1   X   0   X   X \n",
      "\t  X  -1   0   0   0   X   0  -1  -1   X \n",
      "\t  X   X   X   0   0   X   0   X   X   X \n",
      "\t  X   X  -1   X   0  -1  -1   0   0   X \n",
      "\t  X   X   X   X   X   X   X   X   @   X \n",
      "current step: 54 \t score: 96.0\n",
      "\n",
      "Goal!\n"
     ]
    }
   ],
   "source": [
    "QL_solver.qlearn(greedy_flg=True)#####implement greedy policy and display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
